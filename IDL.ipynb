{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bc773f12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "643ecb43",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.models import Sequential\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import pandas as pd\n",
    "from tensorflow.keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c6d76b56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5 1]\n",
      "9\n"
     ]
    }
   ],
   "source": [
    "# Forward Propagation\n",
    "input_data =  np.array([2,3])\n",
    "weights ={'node_0': np.array([1,1]), 'node_1':np.array([-1,1]), 'output': np.array([2,-1])}\n",
    "node_0_value = (input_data * weights['node_0']).sum()\n",
    "node_1_value = (input_data * weights['node_1']).sum()\n",
    "\n",
    "hidden_layer_values = np.array([node_0_value,node_1_value])\n",
    "print(hidden_layer_values)\n",
    "\n",
    "output = (hidden_layer_values * weights['output']).sum()\n",
    "print(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "15cea591",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9901095378334199\n"
     ]
    }
   ],
   "source": [
    "# Activation Function\n",
    "#allows the hidden layers to capture non-linearity\n",
    "\n",
    "input_data =  np.array([-1,2])\n",
    "weights ={'node_0': np.array([3,3]), 'node_1':np.array([1,5]), 'output': np.array([2,-1])}\n",
    "node_0_input = (input_data * weights['node_0']).sum()\n",
    "node_0_output = np.tanh(node_0_input)\n",
    "\n",
    "node_1_input = (input_data * weights['node_1']).sum()\n",
    "node_1_output = np.tanh(node_1_input)\n",
    "\n",
    "hidden_layer_outputs = np.array([node_0_output, node_1_output])\n",
    "output = (hidden_layer_outputs * weights['output']).sum()\n",
    "\n",
    "print(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "57964da8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-3\n"
     ]
    }
   ],
   "source": [
    "# ReLU - rectified linear activation\n",
    "# 0 if x<0 and x if x>=0\n",
    "\n",
    "def relu(input):\n",
    "    output = max(0, input)\n",
    "    return(output)\n",
    "\n",
    "node_0_input = (input_data * weights['node_0']).sum()\n",
    "node_0_output = relu(node_0_input)\n",
    "\n",
    "node_1_input = (input_data * weights['node_1']).sum()\n",
    "node_1_output = relu(node_1_input)\n",
    "\n",
    "hidden_layer_outputs = np.array([node_0_output, node_1_output])\n",
    "\n",
    "model_output = (hidden_layer_outputs * weights['output']).sum()\n",
    "\n",
    "print(model_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9decec09",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_with_network_1(input_data_row, weights):\n",
    "    node_0_input = (input_data_row * weights['node_0']).sum() \n",
    "    node_0_output = relu(node_0_input)\n",
    "    \n",
    "    node_1_input = (input_data_row * weights['node_1']).sum()\n",
    "    node_1_output = relu(node_1_input)\n",
    "    \n",
    "    hidden_layer_outputs = np.array([node_0_output, node_1_output])\n",
    "    \n",
    "    input_to_final_layer = (hidden_layer_outputs * weights['output']).sum()\n",
    "    model_output = relu(input_to_final_layer)\n",
    "    \n",
    "    return(model_output)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "97edb9b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[52, 63, 0, 148]\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "\n",
    "input_data = [np.array([3,5]),np.array([1,-1]),np.array([0,0]),np.array([8,4])]\n",
    "weights = {'node_0': np.array([2, 4]), 'node_1': np.array([ 4, -5]), 'output': np.array([2, 7])}\n",
    "\n",
    "for input_data_row in input_data:\n",
    "    results.append(predict_with_network_1(input_data_row, weights))\n",
    "    \n",
    "print(results)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "effe7b5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deeper Networks - Multiple hidden layers\n",
    "# internally build representations of patterns in the data\n",
    "# partially replace the need for feature engineering\n",
    "# subsequent layers build increasingly sophisticated representations of raw data\n",
    "\n",
    "# Modeler doesn't need to specify the interactions\n",
    "# when the model is trained, the neural network gets weights that find relevant patterns to make better predictions\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0630ff52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "182\n"
     ]
    }
   ],
   "source": [
    "weights = {'node_0_0': np.array([2, 4]), 'node_0_1': np.array([ 4, -5]), 'node_1_0': np.array([-1,  2]), 'node_1_1': np.array([1, 2]), 'output': np.array([2, 7])}\n",
    "\n",
    "\n",
    "def predict_with_network(input_data):\n",
    "    # Calculate node 0 in the first hidden layer\n",
    "    node_0_0_input = (input_data * weights['node_0_0']).sum()\n",
    "    node_0_0_output = relu(node_0_0_input)\n",
    "\n",
    "    # Calculate node 1 in the first hidden layer\n",
    "    node_0_1_input = (input_data * weights['node_0_1']).sum()\n",
    "    node_0_1_output = relu(node_0_1_input)\n",
    "\n",
    "    # Put node values into array: hidden_0_outputs\n",
    "    hidden_0_outputs = np.array([node_0_0_output, node_0_1_output])\n",
    "\n",
    "    # Calculate node 0 in the second hidden layer\n",
    "    node_1_0_input = (hidden_0_outputs * weights['node_1_0']).sum()\n",
    "    node_1_0_output = relu(node_1_0_input)\n",
    "\n",
    "    # Calculate node 1 in the second hidden layer\n",
    "    node_1_1_input = (hidden_0_outputs * weights['node_1_1']).sum()\n",
    "    node_1_1_output = relu(node_1_1_input)\n",
    "\n",
    "    # Put node values into array: hidden_1_outputs\n",
    "    hidden_1_outputs = np.array([node_1_0_output, node_1_1_output])\n",
    "    \n",
    "    # Calculate output here: model_output\n",
    "    model_output = (hidden_1_outputs * weights['output']).sum()\n",
    "    \n",
    "    # Return model_output\n",
    "    return(model_output)\n",
    "\n",
    "input_data = np.array([3,5])\n",
    "output = predict_with_network(input_data)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "316e225f",
   "metadata": {},
   "source": [
    "# Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4684ced8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss function aggregates errors in predictions from data points into single number\n",
    "# it predicts model's predictive performance\n",
    "# common loss function is MSE\n",
    "# lower loss - better model\n",
    "# goal - finding weights that give the lowest value for the loss function\n",
    "# this is done using an algorithm called \"Gradient descent\"\n",
    "\n",
    "# \"How Gradient Descent works\"\n",
    "# 1. Start at a random point\n",
    "# 2. Until you rae somewhat flat, \n",
    "    # find the slope\n",
    "    # take a step downhill\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f2519148",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "input_data = np.array([0, 3])\n",
    "\n",
    "weights_0 = {'node_0': [2, 1],'node_1': [1, 2],'output': [1, 1]}\n",
    "\n",
    "target_actual = 3\n",
    "\n",
    "model_output_0 = predict_with_network_1(input_data, weights_0)\n",
    "\n",
    "error_0 = model_output_0 - target_actual\n",
    "\n",
    "weights_1 = {'node_0': [2, 1],\n",
    "             'node_1': [1, 0],\n",
    "             'output': [1, 1]\n",
    "            }\n",
    "\n",
    "model_output_1 = predict_with_network_1(input_data, weights_1)\n",
    "\n",
    "error_1 = target_actual - model_output_1\n",
    "\n",
    "print(error_0)\n",
    "print(error_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "444cf981",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean squared error with weights_0: 37.500000\n",
      "Mean squared error with weights_1: 49.890625\n"
     ]
    }
   ],
   "source": [
    "target_actuals = np.array([1, 3, 5, 7])\n",
    "weights_0 = {'node_0': np.array([2, 1]), 'node_1': np.array([1, 2]), 'output': np.array([1, 1])}\n",
    "weights_1 = {'node_0': np.array([2, 1]), 'node_1': np.array([1. , 1.5]), 'output': np.array([1. , 1.5])}\n",
    "input_data = [np.array([0, 3]), np.array([1, 2]), np.array([-1, -2]), np.array([4, 0])]\n",
    "\n",
    "model_output_0 = []\n",
    "model_output_1 = []\n",
    "\n",
    "for row in input_data:\n",
    "    # Append prediction to model_output_0\n",
    "    model_output_0.append(predict_with_network_1(row,weights_0 ))\n",
    "    \n",
    "    # Append prediction to model_output_1\n",
    "    model_output_1.append(predict_with_network_1(row,weights_1 ))\n",
    "    \n",
    "\n",
    "mse_0 = mean_squared_error(target_actuals, model_output_0)\n",
    "\n",
    "mse_1 = mean_squared_error(target_actuals, model_output_1)\n",
    "\n",
    "# Print mse_0 and mse_1\n",
    "print(\"Mean squared error with weights_0: %f\" %mse_0)\n",
    "print(\"Mean squared error with weights_1: %f\" %mse_1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a1458bfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient descent\n",
    "# if slope is positive, going opposite the slope - moving to lower numbers\n",
    "# subtract the slope from the current value\n",
    "# too big a step might lead us astray\n",
    "\n",
    "# instead of subtracting the slope, we multiply it by a small number called 'learning rate'\n",
    "# then update each weight by subtracting learning rate * slope\n",
    "\n",
    "# Slope calculation Eg\n",
    "# 3-----2----->6  ;  3 & 6 nodes ; 2 - weight; actual target =10 \n",
    "# To calculate the slope for a weight, need to multiply 3 things\n",
    "#     slope of loss fun wrt value at node we feed into(the model's prediction)\n",
    "#         2(Predicted value - Actual Value) = 2 (6 - 10) = -8\n",
    "#     value at the node that feeds into our weight\n",
    "#         3\n",
    "#     slope of activation fun wrt value we feed into\n",
    "#         we don't have activation function in this case\n",
    "\n",
    "# Multiplying the three(two) -> -8 * 3 = -24(slope of the loss)\n",
    "# If learning rate = 0.01; new weight would be 2-(0.01 * -24 ) = 2.24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "18c93922",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    }
   ],
   "source": [
    "# Network with two inputs affecting prediction\n",
    "\n",
    "# 3 & 4 - nodes; 1 & 2 - weights \n",
    "\n",
    "# 3\n",
    "#  * \n",
    "#   1\n",
    "#    *\n",
    "#     *\n",
    "#    *\n",
    "#   2\n",
    "#  *  \n",
    "# 4\n",
    "\n",
    "weights = np.array([1,2])\n",
    "input_data = np.array([3,4])\n",
    "target = 6\n",
    "learning_rate = 0.01\n",
    "preds = (weights * input_data).sum()\n",
    "error = preds- target\n",
    "print(error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "372083e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[30 40]\n"
     ]
    }
   ],
   "source": [
    "gradient = 2 * input_data*error\n",
    "print(gradient)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "732c3dee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.5\n"
     ]
    }
   ],
   "source": [
    "weights_updated = weights - learning_rate * gradient\n",
    "preds_updated = (weights_updated * input_data).sum()\n",
    "error_updated = preds_updated - target\n",
    "print(error_updated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "800d9037",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Slope :  [14 28 42]\n",
      "error :  7\n",
      "error_updated :  5.04\n"
     ]
    }
   ],
   "source": [
    "\n",
    "weights = np.array([0,2,1])\n",
    "input_data = np.array([1,2,3])\n",
    "target = 0\n",
    "\n",
    "preds = (weights * input_data).sum()\n",
    "error = preds - target\n",
    "slope = 2 * input_data * error\n",
    "\n",
    "print(\"Slope : \",slope)\n",
    "\n",
    "learning_rate = 0.01\n",
    "weights_updated = weights - learning_rate*slope\n",
    "preds_updated = (weights_updated * input_data).sum()\n",
    "error_updated = preds_updated - target\n",
    "\n",
    "print(\"error : \",error)\n",
    "print(\"error_updated : \",error_updated)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d17a0c87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Making multiple updates to weights\n",
    "# n_updates = 20\n",
    "# mse_hist = []\n",
    "\n",
    "# def get_slope(input_data, target, weights):\n",
    "#     preds = (weights * input_data).sum()\n",
    "#     error = preds - target\n",
    "#     slope = 2 * input_data * error\n",
    "#     return slope\n",
    "\n",
    "# def get_mse(input_data, target, weights):\n",
    "# #     model_output = []\n",
    "# #     for row in input_data:\n",
    "# #         model_output.append(predict_with_network_1(row,weights))\n",
    "#     model_output = (input_data*weights).sum()\n",
    "#     mse = mean_squared_error(target, model_output)\n",
    "#     return mse\n",
    "    \n",
    "\n",
    "# n_updates = 20\n",
    "# mse_hist = []\n",
    "\n",
    "# # Iterate over the number of updates\n",
    "# for i in range(n_updates):\n",
    "#     # Calculate the slope: slope\n",
    "#     slope = get_slope(input_data, target, weights)\n",
    "    \n",
    "#     # Update the weights: weights\n",
    "#     weights = weights - slope * 0.01\n",
    "    \n",
    "#     # Calculate mse with new weights: mse\n",
    "#     mse = get_mse(input_data, target, weights)\n",
    "    \n",
    "#     # Append the mse to mse_hist\n",
    "#     mse_hist.append(mse)\n",
    "\n",
    "# # Plot the mse history\n",
    "# plt.plot(mse_hist)\n",
    "# plt.xlabel('Iterations')\n",
    "# plt.ylabel('Mean Squared Error')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a1840272",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Backpropagation\n",
    "# takes the error from the o/p layer and propagates it backwards through the hidden layerstowards the input layer\n",
    "\n",
    "# allows gradient descent to update all weights in neural network (by getting gradients for all weights)\n",
    "# comes from chain rule of calculus\n",
    "\n",
    "# we are trying to estimate the slope of the loss function w.r.t each weight\n",
    "# Do fwd propagation to calculate predictions nad errors before we do back propagation\n",
    "\n",
    "# In back propagation, we go back one layer at a time and Gradients for weight is product of\n",
    "#     node value feeding into that weight\n",
    "#     slope of loss fun w.r.t node it feeds into\n",
    "#     slope of activation fun at the node it feeds into\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2c7c09fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Backpropagation\n",
    "# use fwd propagation to make prediction\n",
    "# use back propagation to calculate the slope of the loss fun w.r.t each weight\n",
    "# multiply that slope by the learning rate and subtract from the current weights\n",
    "# keep going with that cycle until we get to a flat part \n",
    "\n",
    "\n",
    "# Stochastic gradient descent\n",
    "# for computational efficiency, it's common to calculate slopes on only a subset of the data(a batch)\n",
    "# start over from the beginning once all data is used\n",
    "# each time through the full training data is called epoch (1st, 2nd, 3rd,.. epoch)\n",
    "# when slopes are calulated on one batch at a time rather than on full data : stochastic gradient descent\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a20ebaed",
   "metadata": {},
   "source": [
    "# Building DL models with keras "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ee95af83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keras workflow has 4 steps\n",
    "#     specify architecture(no layers, no nodes in layers, activation funs, ..)\n",
    "#     compile(loss function, optimization, ..)\n",
    "#     fit(back propagation and optimization of model weights)\n",
    "#     predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "8e5b52e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n"
     ]
    }
   ],
   "source": [
    "# Model specification\n",
    "predictors = np.loadtxt('hourly_wages.csv', delimiter=',', skiprows=1)[:,1:]\n",
    "n_cols = predictors.shape[1]\n",
    "target = np.loadtxt('hourly_wages.csv', delimiter=',', skiprows=1, usecols=0)\n",
    "\n",
    "print(n_cols)\n",
    "model = Sequential()\n",
    "\n",
    "# first layer\n",
    "model.add(Dense(100, activation='relu',input_shape = (n_cols,)))\n",
    "\n",
    "# second layer\n",
    "model.add(Dense(100, activation='relu'))\n",
    "\n",
    "# output layer\n",
    "model.add(Dense(1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "190c4ad7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss function: mean_squared_error\n",
      "17/17 [==============================] - 1s 5ms/step - loss: 23.5772\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1b65d468760>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compiling model\n",
    "\n",
    "# we can specify optimizer(Usually \"Adam\") and loss function(could be MSE)\n",
    "\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "print(\"Loss function: \" + model.loss)\n",
    "\n",
    "# Fitting model\n",
    "\n",
    "# applying backpropagation and gradient descent with your data to update weights\n",
    "# scaling data before fitting can ease optimization\n",
    "#     one common approach - subtract each feature by that features mean and divide it by its standard deviation\n",
    "\n",
    "model.fit(predictors, target)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "a3f89644",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28/28 [==============================] - 1s 3ms/step - loss: 1.2121 - accuracy: 0.6375\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1b65f9392e0>"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Classification Models\n",
    "\n",
    "# we do couple of things differently from regression in DL\n",
    "#     set loss function as categorical_crossentropy(lower score - beeter) instead of mse\n",
    "#         add metrics= accuracy to compile step for easy-to-understand diagnosis\n",
    "#     output layer has separate node for each possible o/come, and uses 'softmax' activation\n",
    "\n",
    "data =  pd.read_csv('titanic_all_numeric.csv')\n",
    "data[\"age_was_missing\"] = data[\"age_was_missing\"].astype(int)\n",
    "predictors =  data.drop(['survived'], axis=1).values\n",
    "target = to_categorical(data['survived'])\n",
    "\n",
    "n_cols = predictors.shape[1]\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(100, activation='relu', input_shape = (n_cols,)))\n",
    "model.add(Dense(100, activation='relu'))\n",
    "model.add(Dense(100, activation='relu'))\n",
    "model.add(Dense(2, activation= 'softmax'))\n",
    "\n",
    "model.compile(optimizer='sgd', loss= 'categorical_crossentropy', metrics = ['accuracy'])\n",
    "model.fit(predictors, target)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fca3eace",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using models\n",
    "#     save, reload and use the model\n",
    "\n",
    "model.save('model_file.h5')\n",
    "my_model = load_model('model_file.h5')\n",
    "\n",
    "data_to_predict_with = \n",
    "\n",
    "predictions = my_model.predict(data_to_predict_with)\n",
    "probability_true = predictions[:, 1]\n",
    "\n",
    "print(probability_true)\n",
    "# verifying model structure\n",
    "my_model.summary()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
